# general settings
task: GPEN_gan
model: gan # psnr | gan
gpu_ids: [0]
dist: false
n_channels: 3 # broadcast to "datasets", 1 for grayscale, 3 for color

path:
  root: face2face
  pretrained_netG: ~ # path of pretrained model
  pretrained_netE: ~ # path of pretrained model

# datasets settings
datasets:
  train: 
    name: train_dataset
    type: face
    dataroot_hq: /home/viliar/data/DatasetHappy/train_B
    dataroot_lq: /home/viliar/data/DatasetHappy/train_A

    patch_size: 512
    n_channels: 3

    # data loader
    dataloader_shuffle: true
    dataloader_batch_size: 6
    dataloader_num_workers: 6

  test:
    name: test_dataset
    type: face
    dataroot_hq: /home/viliar/Documents/git/MobileImg2Img/TestHappy/train_B
    dataroot_lq: /home/viliar/Documents/git/MobileImg2Img/TestHappy/train_A

    n_channels: 3

netG:
  net_type: u2net
  in_channels: 3
  out_channels: 3

  init_type: normal 
  init_bn_type: uniform
  init_gain: 0.2

netD:
  net_type: discriminator_unet # discriminator_patchgan | discriminator_unet
  in_nc: 3
  base_nc: 64
  n_layers: 3 # only for "net_type":"discriminator_patchgan"
  norm_type: spectral # only for "net_type":"discriminator_patchgan"  | 'batch', 'instance', 'spectral', 'batchspectral', 'instancespectral'

  init_type: orthogonal # "orthogonal" | "normal" | "uniform" | "xavier_normal" | "xavier_uniform" | "kaiming_normal" | "kaiming_uniform"
  init_bn_type: uniform # "uniform" | "constant"
  init_gain: 0.2


train:
  G_lossfn_type: l1 # "l1" | "l2" | "l2sum" | "l1c" | "ssim"
  G_lossfn_weight: 1

  F_lossfn_type: l1 # "l1" | "l2"
  F_lossfn_weight: 1
  F_feature_layer: [2,7,16,25,34]  # 25 | [2,7,16,25,34]
  F_weights: [0.1,0.1,1.0,1.0,1.0]  # 1.0 | [0.1,0.1,1.0,1.0,1.0]
  F_use_input_norm: true
  F_use_range_norm: false

  gan_type: gan # "gan" | "ragan" | "lsgan" | "wgan" | "softplusgan"
  D_lossfn_weight: 0.1

  E_decay: 0.999 # Exponential Moving Average for netG: set 0 to disable; default setting 0.999

  D_init_iters: 0

  G_optimizer_type: adam
  G_optimizer_lr: !!float 1e-4 # learning rate
  G_optimizer_wd: 0

  D_optimizer_type: adam
  D_optimizer_lr: !!float 1e-4 # learning rate
  D_optimizer_wd: 0

  G_scheduler_type: MultiStepLR
  G_scheduler_milestones: [20000, 30000, 40000, 50000, 60000]
  G_scheduler_gamma: 0.5
  G_optimizer_reuse: true

  D_scheduler_type: MultiStepLR
  D_scheduler_milestones: [20000, 30000, 40000, 50000, 60000]
  D_scheduler_gamma: 0.5
  D_optimizer_reuse: false

  G_param_strict: true
  D_param_strict: true
  E_param_strict: true

  checkpoint_test: 3000
  checkpoint_save: 5000
  checkpoint_print: 500