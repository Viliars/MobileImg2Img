# general settings
task: UNet_psnr
model: psnr # psnr | gan
gpu_ids: [0]
dist: false
n_channels: 3 # broadcast to "datasets", 1 for grayscale, 3 for color

path:
  root: face2face
  pretrained_netG: ~ # path of pretrained model
  pretrained_netE: ~ # path of pretrained model

# datasets settings
datasets:
  train: 
    name: train_dataset
    type: face
    dataroot_hq: /home/viliar/data/DatasetHappy/train_B
    dataroot_lq: /home/viliar/data/DatasetHappy/train_A

    patch_size: 512
    n_channels: 3

    # data loader
    dataloader_shuffle: true
    dataloader_batch_size: 12
    dataloader_num_workers: 6

  test:
    name: test_dataset
    type: face
    dataroot_hq: /home/viliar/Documents/git/MobileImg2Img/TestHappy/train_B
    dataroot_lq: /home/viliar/Documents/git/MobileImg2Img/TestHappy/train_A

    n_channels: 3


netG:
  net_type: unet
  in_channels: 3
  out_channels: 3
  dim: 32

  init_type: normal 
  init_bn_type: uniform 
  init_gain: 0.2

  
train:
  G_lossfn_type: l1 # "l1" preferred | "l2sum" | "l2" | "ssim" | "charbonnier"
  G_lossfn_weight: 1.0 # default
  E_decay: 0.999 # Exponential Moving Average for netG: set 0 to disable; default setting 0.999

  G_optimizer_type: adam # fixed, adam is enough
  G_optimizer_lr: !!float 2e-4 # learning rate
  G_optimizer_wd: 0 # weight decay, default 0
  G_optimizer_clipgrad: ~ # unused
  G_optimizer_reuse: true #

  G_scheduler_type: MultiStepLR # "MultiStepLR" is enough
  G_scheduler_milestones: [2000, 3000, 4000, 5000, 6000]
  G_scheduler_gamma: 0.5

  G_regularizer_orthstep: ~ # unused
  G_regularizer_clipstep: ~ # unused

  G_param_strict: true
  E_param_strict: true

  checkpoint_test: 1000 # for testing
  checkpoint_save: 1000 # for saving model
  checkpoint_print: 100 # for print
  